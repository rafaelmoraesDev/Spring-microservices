<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Tips for upgrading H2 database in your test-suite</title><link rel="alternate" href="https://blog.kie.org/2023/08/tip-h2-upgrade.html" /><author><name>Gonzalo Muñoz Fernández</name></author><id>https://blog.kie.org/2023/08/tip-h2-upgrade.html</id><updated>2023-08-03T09:04:12Z</updated><content type="html">“Companies pay too much attention to the cost of doing something. They should worry more about the cost of not doing it.” (Philip Kotler) In the world of IT, there’s an old adage that states: "Months of debugging can save days of reading the documentation." Jokes aside, while documentation is crucial for understanding software libraries, it may not always provide all the answers, especially when upgrading a library to newer versions. Unexpected issues and regressions can arise, requiring additional help from the developer community. In this brief post, we aim to give some tips for H2 upgrade (based on ) with the purpose of assisting other developers facing challenges when upgrading the H2 library from an early version (e.g., 1.4.19x) to a newer release (e.g., 2.2.220, as in our case).  The H2 in-memory database is widely used for persistence tests in KIE projects and hadn’t been updated to recent versions for years. As a result, we foresaw that there would be a significant challenge. “Between version 1.4.200 and version 2.0.202 there have been considerable changes, such that a simple update is not possible” , and we were even before that point. So hands-on, and let’s dive in and explore some tips for H2 upgrade with samples and considerations we followed for a smooth transition. FIRST TIP: YOU MAY HAVE RESERVED WORDS IN YOUR TABLES Our first step was to delete "MVCC=true" from the JDBC URL parameters and add "MODE=LEGACY" to keep the compatibility. However, this was not enough as the following exception was raised. org.h2.jdbc.JdbcSQLSyntaxErrorException: Table "CORRELATIONPROPERTYINFO" not found; SQL statement: insert into CorrelationPropertyInfo (correlationKey_keyId, name, value, OPTLOCK, propertyId) values (?, ?, ?, ?, ?) [42102-220] Diving in the logs, we found the culprit: Caused by: org.h2.jdbc.JdbcSQLSyntaxErrorException: Syntax error in SQL statement "create table CorrelationPropertyInfo (propertyId bigint generated by default as identity, name varchar(255), [*]value varchar(255), OPTLOCK integer, correlationKey_keyId bigint, primary key (propertyId))"; expected "identifier"; SQL statement: So, the exception message states that the CorrelationPropertyInfo table cannot be created. The root cause is this table has a column named "value" and this is a reserved word for H2; therefore, it fails from version 200. In this case, H2 documentation mentions a workaround we can use: “ that can be used as a temporary workaround if your application uses them as unquoted identifiers.” For a quick replacement in all files, we used this “sed” command: find ./ -type f -exec sed -i 's/MVCC=[Tt][Rr][Uu][Ee]/MODE=LEGACY;NON_KEYWORDS=VALUE/g' {} \; SECOND TIP: HIBERNATE PERHAPS NEED TO BE UPGRADED TOO The next challenge was a little bit difficult to find out. We realized that database cleanup was not correctly done, and after activating logs we saw this exception in tests: h2 SchemaDropperImpl$DelayedDropActionImpl] ERROR HHH000478: Unsuccessful: drop table if exists This hibernate issue explains the details about it, and the disruptive in H2 that provokes it.   Therefore, we need to upgrade hibernate-core to at least or . NOTE: In case your code contains some of the StandardDialectResolver constants, notice that they in 5.4.0 as stated in . Then, you will have to replace: DialectResolver resolver = StandardDialectResolver.INSTANCE; by DialectResolver resolver = new StandardDialectResolver(); THIRD TIP: DROP-SOURCE STRATEGY SWITCHES TO “SCRIPT” WITH DROP ALL OBJECTS In our container tests, however, the previous tip was not easy to apply. Wildfly version 23 is not updated yet to the hibernate-core library version that we need: ./cargo/installs/wildfly-dist-23.0.0.Final/wildfly-23.0.0.Final/modules/system/layers/base/org/hibernate/main/hibernate-core-5.3.20.Final.jar So what to do in this case? Instead of relying on metadata for dropping the tables (default behavior), let’s make a trick and set that the schema shall be dropped based on a script. We need to add these two properties with their corresponding variables: &lt;property name="javax.persistence.schema-generation.drop-source" value="${org.jbpm.drop.source}" /&gt; &lt;property name="javax.persistence.schema-generation.drop-script-source" value="${org.jbpm.drop.script}" /&gt; Where default values are defined as: &lt;org.jbpm.drop.source&gt;script&lt;/org.jbpm.drop.source&gt; &lt;org.jbpm.drop.script&gt;${project.basedir}/src/test/resources/drop-tables.sql&lt;/org.jbpm.drop.script&gt; Notice that they can be overridden for keeping the same behavior for the rest of the databases and path must be absolute. And for H2, drop-tables.sql is not considering a CASCADE strategy but a single-shot action, avoiding the issue with the foreign keys: DROP ALL OBJECTS FOURTH TIP: CREATING DATABASE FROM TCP CONNECTION NEEDS “IFNOTEXISTS” The next error was for those databases created remotely from a TCP connection. Exception showed us that the database didn’t exist: Database … not found, either pre-create it or allow remote database creation (not recommended in secure environments) Older versions of H2 () allowed the database creation by default, introducing a security hole in your system. So, we need to set up the parameter "ifNotExists" explicitly when . Again, with the "sed" command, we can rapidly modify all files containing that pattern: find ./ -type f -exec sed -i 's/createTcpServer(new String\[0\])/createTcpServer(new String\[\]{"-ifNotExists"})/g' {} \; NOTE: Take into account this advice from : "Its combination with -tcpAllowOthers, -pgAllowOthers, or -webAllowOthers effectively creates a remote security hole in your system, if you use it, always guard your ports with a firewall or some other solution and use such combination of settings only in trusted networks.” CONCLUSION For many reasons (security and bug fixes, performance improvements, compatibility, new features, community support, and ecosystem compatibility) is very important to upgrade open-source libraries to the latest stable version. In this post, we have followed the process for stabilizing our tests after upgrading H2 from 1.4.197 (set two years ago) to the 2.2.220 version.  We will be delighted if any of the tips for H2 upgrade shared here prove helpful to the community. Happy upgraded testing! The post appeared first on .</content><dc:creator>Gonzalo Muñoz Fernández</dc:creator></entry><entry><title>3 reasons to drop Docker for Podman</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/03/3-reasons-drop-docker-podman" /><author><name>Aine Keenan</name></author><id>2acf5595-4721-4ec9-9ad8-bc740ec57fe1</id><updated>2023-08-03T07:00:00Z</updated><published>2023-08-03T07:00:00Z</published><summary type="html">&lt;p&gt;Many think of Podman to be a replacement for Docker (if they have heard of Podman at all). But, this is not the case, as Podman is another option that provides better &lt;a href="https://developers.redhat.com/topics/security/"&gt;security&lt;/a&gt; and developer features. Podman is a cloud-native, daemonless tool that helps developers manage their &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;. Podman is all about security, but also minimizing the friction between your local development environment and production.&lt;/p&gt; &lt;p&gt;Podman uses a &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt; approach, creating a network with many other cloud-native products, such as Buildah and Skopeo, to build and push containers. This makes Podman a lighter and faster application than Docker, allowing for customization and changes.&lt;/p&gt; &lt;p&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;In this article, we will describe three advantages of Podman related to extensions and embedded tools integrated in the Podman Desktop, as well as the underlying technologies for the container engine.&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;#1: Podman makes creating pods easy&lt;/h2&gt; &lt;p&gt;Creating pods, or podify, is a feature that allows you to combine running containers to create pods. It is as simple as selecting the containers you need, clicking the pod symbol, and customizing the pod. You can choose the name, the ports you would like exposed, and more (Figure 1). The ease of creating pods is unmatched by any other container engine, whether CLI or GUI.&lt;/p&gt; &lt;p&gt;After your pod is created, you will see it in the container and pod section. Developers have access to a variety of details about the pod, just like for any &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster. First, you will be greeted with a summary and logs. In the inspect section, you can access all low-level information about your pod, including the infrastructure container ID, all container information, network and namespace options, and more.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/podify.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/podify.png?itok=V37ZRKaX" width="600" height="419" alt="A screenshot of Podman Desktop for creating a pod." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Creating a pod using Podman Desktop.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;#2: Podman's Kubeify feature&lt;/h2&gt; &lt;p&gt;The most captivating feature of Podman Desktop is the Kubeify feature. Unlike other OCI container engines, there is zero friction moving from your container, pod, or volume to a Kubernetes pod. For any item, developers simply look under the Kube section, and they will be greeted with a Kubernetes manifest to make that item a Kubernetes pod (Figure 2). If you feel more comfortable with the CLI, the command &lt;code&gt;podman generate kube &lt;name of object&gt;&lt;/code&gt; will also work.&lt;/p&gt; &lt;p&gt;Additionally, Podman Desktop takes Kubeify a step further than just generating Kubernetes manifests. Extensions allow developers to connect their Podman Desktop, and therefore all of their manifests, containers, pods, etc, to a cluster of their choice. It comes with three suggested extensions developers can use for a local or remote cluster: the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, Kind, and &lt;a href="https://developers.redhat.com/products/openshift-local/overview"&gt;Red Hat OpenShift Local&lt;/a&gt;. Additionally, if developers have another local or remote cluster they prefer to use, they can input the image name and add it to their desktop. For more information about how to install Podman Desktop extensions, visit the &lt;a href="https://podman-desktop.io/docs/extensions/install"&gt;Podman Desktop site&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;After you have added your preferred local or remote Kubernetes cluster, select which service you would like to deploy and click &lt;strong&gt;Deploy to Kubernetes&lt;/strong&gt;. It is as simple as creating your container/pod, selecting your preferred cluster, and clicking deploy.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kube-ify.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/kube-ify.png?itok=-fIavYkF" width="600" height="419" alt="The Podman Desktop auto-generated Kubernetes manifest for a pod." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The Podman Desktop auto-generated Kubernetes manifest for a pod.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;#3: Better tools and extensions&lt;/h2&gt; &lt;p&gt;Podman offers better tools and extensions, such as pulling images, security, and auditing features.&lt;/p&gt; &lt;h3&gt;Pulling images&lt;/h3&gt; &lt;p&gt;Evidently, Podman and Docker have to allow developers to pull an image, since both products are container engines. You can do this using the terminal.&lt;/p&gt; &lt;p&gt;Unlike Docker, you can pull an image while in the Podman Desktop application. Docker will force you to head to the terminal. While the switch is annoying and can disrupt the workflow, the real kicker is that when using the Docker free plan, there are limited pulls a day unless you pay to upgrade.&lt;/p&gt; &lt;p&gt;With a team of over 500 employees, Docker is able to provide a breadth to their platform for which Podman seems to be aiming. To a developer, the first feature that stands out in Docker is the variety of extensions they offer, totaling over 60. With suggested offerings from MongoDB, Live Charts, Grafana, and more, it is eye-grabbing. Having these features improves developers' experience and removes the friction of adding them. However, if a developer wants to add their own extensions in Docker, it is very tedious, even downright impossible.&lt;/p&gt; &lt;p&gt;While Podman Desktop is lacking in pre-loaded extensions, they allow developers to add their own extensions by inputting the name of the image for the extensions. Any extensions that Docker has can be added to Podman Desktop.&lt;/p&gt; &lt;p&gt;Docker Desktop continues to flex with dev environments and the learning center. Dev environments, currently in Beta, work to have a code editor integrating the web platform. Additionally, they provide many walkthroughs and samples adding to the experience and growth of developers.&lt;/p&gt; &lt;h3&gt;Podman offers better security&lt;/h3&gt; &lt;p&gt;While Podman Desktop and Docker Desktop are the user interfaces, the underlying engines have significant differences as well.&lt;/p&gt; &lt;p&gt;Podman has opted for a rootless model, meaning the container system is, by default, run by a non-root user. Even if a user inside the container is the root user, once they are outside of the container, they have as much access as a non-root. Podman opts to run containers with user namespaces and SELinux containers, so if a container is attacked, the attacker only has access to files in the container. They cannot mess with the host or other containers due to incorrect permissions.&lt;/p&gt; &lt;p&gt;While Docker does support a rootless daemon, by default the Docker daemon runs as root user. Thus, if it is compromised, the attacker has instant access to the entire system. The rootful Docker daemon is a background process on the host, so it needs full access to the system. Since containers are run through the daemon, a malicious container could control the host.&lt;/p&gt; &lt;h3&gt;Auditing features&lt;/h3&gt; &lt;p&gt;Podman has emphasized the importance of security, opting for a fork/execute model, while Docker has a client/server approach.&lt;/p&gt; &lt;p&gt;All Docker commands trigger the Docker client to send a request to the Docker daemon (the server). The Docker daemon is a child of the init system, causing all systemd, daemon, and container processes to share the same login. If a user ID for the init system is unset, all other recorded processes will have an unset user ID, allowing anonymous use.&lt;/p&gt; &lt;p&gt;Podman runs as a process and when given a command, forks itself, and has the new replica execute the new command. Multiple users can run Podman with each process forked and run separately.&lt;/p&gt; &lt;p&gt;Since Podman runs from a process model, all individual processes and containers are recorded. All processes stem from a certain user, meaning Podman’s logs will record a user ID, allowing for more correct and secure auditing.&lt;/p&gt; &lt;h2&gt;Switching to Podman is easy&lt;/h2&gt; &lt;p&gt;&lt;span&gt;Podman offers a variety of other developer tools that make building and pulling an image, playing Kubernetes YAML and pruning unused files a one-stop process. Podman and the desktop work together to ensure Podman is integrated into the Linux ecosystem.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Migrating from Docker to Podman has only gotten easier (as described in this &lt;a href="https://developers.redhat.com/blog/2020/11/19/transitioning-from-docker-to-podman"&gt;article&lt;/a&gt;). Podman Desktop has extensions that allow for migration from Docker and Docker compose to the Podman ecosystem. While Podman may be viewed as similar to Docker, it has many developer, security, and Linux-based features that contribute to its advantage. Consider making the leap from Docker to Podman. With the ease of migration, the leap isn’t far.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/03/3-reasons-drop-docker-podman" title="3 reasons to drop Docker for Podman"&gt;3 reasons to drop Docker for Podman&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Aine Keenan</dc:creator><dc:date>2023-08-03T07:00:00Z</dc:date></entry><entry><title>A beginner's guide to Git version control</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/02/beginners-guide-git-version-control" /><author><name>Mohammadi Iram</name></author><id>f3a9ba14-0e05-43ab-973a-d74583864c6a</id><updated>2023-08-02T07:00:00Z</updated><published>2023-08-02T07:00:00Z</published><summary type="html">&lt;p&gt;Git is a widely used distributed version control system that allows software development teams to have multiple local copies of the project's source code that are independent of each other. Version control has come to be associated with Git—it is unquestionably the best version control program for new developers to start learning due to its popularity and wealth of resources related to its use. Read on for an overview of the basics.&lt;/p&gt; &lt;h2&gt;Git configuration: Linux&lt;/h2&gt; &lt;p&gt;Most Linux installations have Git, but to check, execute the following command in your terminal:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git --version&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should get output that is similar to the following:&lt;/p&gt; &lt;pre&gt; git version 2.40.1&lt;/pre&gt; &lt;h2&gt;Git configuration: Windows&lt;/h2&gt; &lt;p&gt;Git searches the &lt;code&gt;$HOME&lt;/code&gt; directory for the &lt;code&gt;.gitconfig&lt;/code&gt; file on Windows systems.&lt;/p&gt; &lt;p&gt;We need to tell Git to keep track of our login and email when we use it, as shown in the code snippet below. This makes it possible for other code contributors to identify the change's author and our contact information in case of problems.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ git config --global user.name "username" $ git config --global user.email "useremail"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use the following command if you need assistance:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git help config&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command will open a browser containing configuration commands. Essentially, the &lt;code&gt;help&lt;/code&gt; command gives a manual from the help page for the given command.&lt;/p&gt; &lt;p&gt;You can also use the same command in the following ways:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git config --help&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use the following command to view configurations:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git config -l&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Working with repositories&lt;/h2&gt; &lt;p&gt;A directory that Git will track is called a &lt;strong&gt;repository&lt;/strong&gt;,&lt;strong&gt; &lt;/strong&gt;or repo. The Git repository contains the whole of our project. Git will trace any change we make. We'll use the command below to create a test directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir test&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can then use the following command to enter the test directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd test&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Running &lt;code&gt;git init&lt;/code&gt; command inside the directory lets Git know that it is a Git repository:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ git init  Empty Git repository created and initialised in /home/uname/test/.git&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now a Git repository exists in this directory. Showing the &lt;code&gt;.git&lt;/code&gt; file that Git generated inside the directory will be helpful; use the command as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ls -a&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; .git&lt;/pre&gt; &lt;p&gt;The directory is now a Git repository. In fact, deleting the &lt;code&gt;.git&lt;/code&gt; directory will uninitialize the repository (this can be very helpful when you're just starting out). Run the following command to make your directory a non-Git repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ rm.git -rf&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Use this only if you truly want to start over because it will remove Git from the directory completely.&lt;/p&gt; &lt;p&gt;Let's create two files in the test directory with the names &lt;code&gt;file1&lt;/code&gt; and &lt;code&gt;file2&lt;/code&gt;. This will show how Git tracks files:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ touch file1.txt $ touch file2.txt&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Staging files: git add &lt;/h2&gt; &lt;p&gt;The principles of the staging environment and the commit are essential to Git. You can add, change, or remove files as you work. However, anytime you reach an important stage or complete a portion of the work, you should move the files to a staging environment. Files that have been &lt;strong&gt;staged&lt;/strong&gt; are ready to be committed to the repository you are working on. (We'll discuss commits in more detail in a later section.)&lt;/p&gt; &lt;p&gt;We can see what Git is tracking using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ git status On branch main No commits yet Untracked files:   (use "git add &lt;file&gt;..." to include in what will be committed)         file1.txt         File2.txt&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;git status&lt;/code&gt; displays a list of newly added or modified files and directories in the Git repository. In our example, &lt;code&gt;file1.txt&lt;/code&gt; and &lt;code&gt;file2.txt&lt;/code&gt; have been modified.&lt;/p&gt; &lt;p&gt;We must stage the files in order to instruct Git to keep track of changes. Let's use the &lt;code&gt;add&lt;/code&gt; command to stage &lt;code&gt;file1.txt&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ git add file1.txt $ git status On branch master No commits yet Changes to be committed:    (use "git rm --cached &lt;file&gt;..." to unstage)         new file: file1.txt Untracked files:   (use "git add &lt;file&gt;..." to include in what will be committed)         file2.txt&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice that &lt;code&gt;file1.txt&lt;/code&gt; is listed under &lt;strong&gt;Changes to be committed&lt;/strong&gt; by Git. This indicates that Git is keeping note of any modifications made to this file in order to commit those modifications to the repository. Let's add &lt;code&gt;file2.txt&lt;/code&gt; now:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ git add file2.txt $ git status ... Changes to be committed:   (use "git rm --cached &lt;file&gt;..." to unstage)         new file: file1.txt         new file: file2.txt&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can instruct Git to monitor a file or directory using the &lt;code&gt;add&lt;/code&gt; command or to stop tracking a file or directory with the &lt;code&gt;unstaging&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;The git &lt;code&gt;rm&lt;/code&gt; command is used to remove individual files or a set of files using the filename, as shown below. The &lt;code&gt;git rm --cached&lt;/code&gt; command maintains a file in the working directory while removing it from the Git index. &lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ git rm --cached file1.txt rm file1.txt $ git status … Untracked files: (   use "git add &lt;file&gt;..." to include in what will be committed)         file1.txt &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;file1.txt&lt;/code&gt; file is no longer being tracked.&lt;/p&gt; &lt;h2&gt;Keeping track of untracked files&lt;/h2&gt; &lt;p&gt;We frequently need a quick way to tell Git to add everything that is untracked to the staging area. We can do this by using &lt;code&gt;*&lt;/code&gt; or &lt;code&gt;.&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git add --all&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git add .&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git add *&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Stage all changes (new, changed, and deleted) files by using &lt;code&gt;--all&lt;/code&gt; rather than specific filenames.&lt;/p&gt; &lt;h2&gt;Commits&lt;/h2&gt; &lt;p&gt;Git uses &lt;strong&gt;commits&lt;/strong&gt; to make changes to files and directories permanent. In a sense, every commit represents a new version of our repository. Even while a commit can be seen to be a more permanent change, Git makes it simple to undo those changes, which is the strength of version control with Git.&lt;/p&gt; &lt;p&gt;For Git users, this alters the fundamental development model. Git developers have the option to build up commits in their local repo before making a change and committing it to the main repository. It accomplishes this by tracking the history of commits. Git's main purpose is to allow users to make commits. Everything we wanted to stage has already been done, so we can now make those modifications. &lt;code&gt;git commit&lt;/code&gt; is used to do this.&lt;/p&gt; &lt;p&gt;When using the &lt;code&gt;git commit&lt;/code&gt; command, we recommend always including two arguments: &lt;code&gt;-a&lt;/code&gt; and &lt;code&gt;-m&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Stage all modified files: git commit -a&lt;/h3&gt; &lt;p&gt;Add all untracked files to the staging area with the &lt;code&gt;-a&lt;/code&gt; or &lt;code&gt;--all&lt;/code&gt; option. Note that only previously added files and folders are added using this method. Use the &lt;code&gt;add&lt;/code&gt; command first if a file or directory has to be added that hasn't already been.&lt;/p&gt; &lt;h3&gt;Commit messages: git commit -m&lt;/h3&gt; &lt;p&gt;A commit message should always be included when making a commit. The &lt;code&gt;-m&lt;/code&gt; or &lt;code&gt;--message&lt;/code&gt; option is used for this. This message should be brief and descriptive, with just enough information included in the commit statement to summarize your actions since the last commit.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git commit -am "my first git commit"&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;View change history: git log&lt;/h3&gt; &lt;p&gt;To view the history of changes that have been committed to a Git repository, use the &lt;code&gt;git log&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git log&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In order to make our output easier to read and only to show the first seven characters of the commit ID, we'll also use the option &lt;code&gt;--oneline&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git log -oneline&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Output:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; cf0p490 (HEAD -&gt; main) my first git commit&lt;/pre&gt; &lt;p&gt;In this example, the commit ID's first seven characters are &lt;code&gt;cf0p490&lt;/code&gt;. There will be a lot of commits; thus each one needs to have its own identification. We can move the &lt;code&gt;HEAD&lt;/code&gt; pointer around as necessary.&lt;/p&gt; &lt;h2&gt;Publishing changes: git push&lt;/h2&gt; &lt;p&gt;Finally, the &lt;code&gt;git push&lt;/code&gt; command is used to upload content from a local repository to a remote repository. &lt;strong&gt;Pushing&lt;/strong&gt; refers to the process of transferring commits from your local repository to a remote repository.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git push&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git push origin branchname&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Where to learn more&lt;/h2&gt; &lt;p&gt;Explore more Git resources on Red Hat Developer for new and advanced users:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/cheat-sheets/git-cheat-sheet"&gt;Git cheat sheet&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/07/20/git-workflows-best-practices-gitops-deployments"&gt;Git best practices: Workflows for GitOps deployments&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/02/02/protect-secrets-git-cleansmudge-filter"&gt;Protect secrets in Git with the clean/smudge filter&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2020/02/25/how-to-ignore-files-in-git-without-gitignore"&gt;How to ignore files in Git without .gitignore&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/02/beginners-guide-git-version-control" title="A beginner's guide to Git version control"&gt;A beginner's guide to Git version control&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mohammadi Iram</dc:creator><dc:date>2023-08-02T07:00:00Z</dc:date></entry><entry><title>Quarkus 3.2.3.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-3-2-3-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-3-2-3-final-released/</id><updated>2023-08-02T00:00:00Z</updated><published>2023-08-02T00:00:00Z</published><summary type="html">August is upon us but we are still steadily publishing new Quarkus releases. Today, we released Quarkus 3.2.3.Final, the third maintenance release of our 3.2 release train. It should be a safe upgrade for anyone already using 3.2. Note that this version upgrades Hibernate Search to 6.2 as we wanted...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-08-02T00:00:00Z</dc:date></entry><entry><title>How to deploy the new Grafana Tempo operator on OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/08/01/how-deploy-new-grafana-tempo-operator-openshift" /><author><name>Pavol Loffay</name></author><id>e2c1f40d-0262-4d1e-92a6-f3ac577f6d19</id><updated>2023-08-01T07:00:00Z</updated><published>2023-08-01T07:00:00Z</published><summary type="html">&lt;p&gt;The distributed tracing team at Red Hat is excited to announce the &lt;a href="https://github.com/grafana/tempo-operator"&gt;Tempo operator&lt;/a&gt; technology preview product on Red Hat OpenShift as part of distributed tracing. We have released Tempo operator to all supported OpenShift versions (4.10 and above).&lt;/p&gt; &lt;p&gt;In this article, we will describe the Tempo operator and features available on OpenShift. The Tempo operator is also available to the upstream Kubernetes users through &lt;a href="https://operatorhub.io/operator/tempo-operator"&gt;operatorhub.io&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Why use Grafana Tempo?&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://grafana.com/docs/tempo/latest/"&gt;Tempo documentation&lt;/a&gt; states, “Grafana Tempo is an open source, easy-to-use, and high-volume distributed tracing backend. Tempo is cost-efficient and only requires object storage to operate. Tempo is deeply integrated with Grafana, Mimir, Prometheus, and Loki. You can use Tempo with open-source tracing protocols, including Jaeger, Zipkin, or OpenTelemetry.”&lt;/p&gt; &lt;p&gt;Why Tempo? Tempo aligns well with other Red Hat observability products (e.g., Prometheus and Loki). It also uses object storage to store data, has similar design, and therefore it should be operationally similar to the other observability backends. We believe that Tempo is a good choice, and it will give OpenShift users great user experience and improve observability capabilities.&lt;/p&gt; &lt;p&gt;One of the exciting Tempo features is &lt;a href="https://grafana.com/docs/tempo/latest/traceql/"&gt;TraceQL&lt;/a&gt;. It is a query language inspired by &lt;a href="https://prometheus.io/docs/prometheus/latest/querying/basics/"&gt;PromQL&lt;/a&gt; and &lt;a href="https://grafana.com/docs/loki/latest/logql/"&gt;LogQL&lt;/a&gt;. In the upcoming releases, OpenShift users will be able to use similar language to query all telemetry signals.&lt;/p&gt; &lt;p&gt;Tempo can be deployed in &lt;a href="https://grafana.com/docs/tempo/latest/setup/deployment/#monolithic-mode"&gt;monolithic&lt;/a&gt; and &lt;a href="https://grafana.com/docs/tempo/latest/setup/deployment/#microservices-mode"&gt;microservices&lt;/a&gt; mode. The current Tempo operator supports only the microservices mode, which creates several deployments for Tempo components (Figure 1).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/tempo_arch.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/tempo_arch.png?itok=qH_wI9BC" width="600" height="390" alt="A screenshot of the Grafana Tempo architecture." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The Grafana Tempo architecture.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;The Tempo operator features&lt;/h2&gt; &lt;p&gt;The Tempo operator manages TempoStack &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"&gt;Custom Resource Definition&lt;/a&gt; (CRD) that allows users to create Tempo instances the Kubernetes native way. It simplifies Tempo installation by giving users a curated set of configuration options and hides operational complexity by handling automated upgrades, for instance.&lt;/p&gt; &lt;p&gt;The operator features:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Resource management&lt;/strong&gt;: A single resource definition splitS across all Tempo containers.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Authentication and authorization:&lt;/strong&gt; OIDC with static RBAC or integration with OpenShift OAuth and SubjectAccessReview.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Security:&lt;/strong&gt; Internal communication is protected with mTLS.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Automated monitoring:&lt;/strong&gt; The operator configures Prometheus service monitors to scrape metrics from Tempo deployments.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Networking:&lt;/strong&gt; The Jaeger UI is exposed via the OpenShift route.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;The Tempo user interface&lt;/h3&gt; &lt;p&gt;The upstream Tempo project does not have any native user interface, however the TempoStack can be configured to deploy a Jaeger query with the Jaeger UI. This configuration option not only allows users to use Jaeger UI, but also exposes the Jaeger query APIs.&lt;/p&gt; &lt;p&gt;Grafana users can configure the Tempo datasource and use externally managed Grafana as a visualization tool for Tempo.&lt;/p&gt; &lt;p&gt;The following fraction of the TempoStack CR enables the Jaeger UI:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: tempo.grafana.com/v1alpha1 kind: TempoStack metadata:   name: simplest spec:   template:     queryFrontend:       jaegerQuery:         enabled: true&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Object storage&lt;/h3&gt; &lt;p&gt;The Tempo operator supports OpenShift Data Foundation, AWS S3, Azure, GCS and Minio for storing Tempo data. Follow the &lt;a href="https://tempo-operator.netlify.app/docs/object_storage.md/"&gt;operator docs&lt;/a&gt; to learn how to create a secret for connecting to your preferred storage option.&lt;/p&gt; &lt;h2&gt;How to install the Tempo operator&lt;/h2&gt; &lt;p&gt;On OpenShift, you can install the operator directly from the Operator Hub (Figure 2). The Operator Hub will show two Tempo operators: the community distribution and the supported Red Hat OpenShift distributed tracing product:&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_of_operatorhub_red_hat_openshift_dedicated.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_of_operatorhub_red_hat_openshift_dedicated.jpg?itok=vqCkZ7Oa" width="600" height="280" alt="A screenshot of the Operator Hub page on OpenShift." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-license field--type-entity-reference field--label-inline field__items"&gt; &lt;span class="field__label"&gt;License&lt;/span&gt; &lt;span class="rhd-media-licence field__item"&gt; under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0" title="Apache 2.0"&gt;Apache 2.0&lt;/a&gt;. &lt;/span&gt; &lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The Operator Hub on OpenShift.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;A multitenancy example&lt;/h3&gt; &lt;p&gt;Tempo is a native multitenant system. On OpenShift, the Tempo operator deploys a gateway that uses OpenShift OAuth (reading) and TokenReview (writing) for authentication, and SubjectAccessReview for authorization.&lt;/p&gt; &lt;p&gt;The following TempoStack CR enables authentication with multitenancy and configures two tenants with the names, dev and prod. The example uses S3 object storage with secret tempostack-dev-minio. Refer to the &lt;a href="https://tempo-operator.netlify.app/docs/object_storage.md/"&gt;documentation&lt;/a&gt; to learn how to set it up.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;kubectl apply -f - &lt;&lt;EOF apiVersion: tempo.grafana.com/v1alpha1 kind:  TempoStack metadata:   name: simplest   namespace: observability spec:   storage:     secret:       name: tempostack-dev-minio       type: s3   storageSize: 1Gi   resources:     total:       limits:         memory: 2Gi         cpu: 2000m   tenants:     mode: openshift     authentication:       - tenantName: dev         tenantId: "1610b0c3-c509-4592-a256-a1871353dbfa"       - tenantName: prod         tenantId: "6094b0f1-711d-4395-82c0-30c2720c6648"   template:     gateway:       enabled: true     queryFrontend:       jaegerQuery:         enabled: true EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These are the main objects created by the operator:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;NAME                                              HOST/PORT                                               PATH   SERVICES                 PORT     TERMINATION   WILDCARD route.route.openshift.io/tempo-simplest-gateway   tempo-simplest-gateway-observability.apps-crc.testing          tempo-simplest-gateway   public   passthrough   None NAME                                              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                                             AGE service/tempo-simplest-compactor                  ClusterIP   10.217.5.105   &lt;none&gt;        7946/TCP,3200/TCP                                                   18h service/tempo-simplest-distributor                ClusterIP   10.217.4.38    &lt;none&gt;        4317/TCP,3200/TCP                                                   18h service/tempo-simplest-gateway                    ClusterIP   10.217.5.27    &lt;none&gt;        8090/TCP,8081/TCP,8080/TCP                                          18h service/tempo-simplest-gossip-ring                ClusterIP   None           &lt;none&gt;        7946/TCP                                                            18h service/tempo-simplest-ingester                   ClusterIP   10.217.4.27    &lt;none&gt;        3200/TCP,9095/TCP                                                   18h service/tempo-simplest-querier                    ClusterIP   10.217.4.65    &lt;none&gt;        7946/TCP,3200/TCP,9095/TCP                                          18h service/tempo-simplest-query-frontend             ClusterIP   10.217.4.222   &lt;none&gt;        3200/TCP,9095/TCP,16686/TCP,16687/TCP                               18h service/tempo-simplest-query-frontend-discovery   ClusterIP   None           &lt;none&gt;        3200/TCP,9095/TCP,9096/TCP,16686/TCP,16687/TCP                      18h NAME                                            READY   UP-TO-DATE   AVAILABLE   AGE deployment.apps/tempo-simplest-compactor        1/1     1            1           18h deployment.apps/tempo-simplest-distributor      1/1     1            1           18h deployment.apps/tempo-simplest-gateway          1/1     1            1           18h deployment.apps/tempo-simplest-querier          1/1     1            1           18h deployment.apps/tempo-simplest-query-frontend   1/1     1            1           18h NAME                                       READY   AGE statefulset.apps/tempo-simplest-ingester   1/1     18h&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The operator creates a tempo-simplest-gateway route to access the UI, however we need to create a ClusterRole to allow users accessing the UI. The following ClusterRole gives all OpenShift authenticated users access to the dev tenant. Then the Jaeger UI can be accessed on this URL: https://tempo-simplest-gateway-observability.{OpenShift base domain}/api/traces/v1/dev/search).&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;kubectl apply -f - &lt;&lt;EOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata:   name: tempostack-traces-reader rules:   - apiGroups:       - 'tempo.grafana.com'     resources:       - dev     resourceNames:       - traces     verbs:       - 'get' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:   name: tempostack-traces-reader roleRef:   apiGroup: rbac.authorization.k8s.io   kind: ClusterRole   name: tempostack-traces-reader subjects:   - kind: Group     apiGroup: rbac.authorization.k8s.io     name: system:authenticated EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To write, the data clients (e.g., OpenTelemetry collector) have to send the X-Scope-OrgID header with the tenant name (e.g., dev) and Kubernetes ServiceAccount token (e.g., /var/run/secrets/kubernetes.io/serviceaccount/token) as a bearer token. Therefore, the client’s ServiceAccount has to be associated with the ClusterRole giving write access for a given tenant.&lt;/p&gt; &lt;p&gt;The following example creates an OpenTelemetry collector and configures it to send data to the previously deployed Tempo:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;kubectl apply -f - &lt;&lt;EOF --- apiVersion: opentelemetry.io/v1alpha1 kind: OpenTelemetryCollector metadata:   name: dev   namespace: observability spec:   config: |     extensions:       bearertokenauth:         filename: "/var/run/secrets/kubernetes.io/serviceaccount/token"     receivers:       otlp:         protocols:           grpc:           http:       jaeger:         protocols:           thrift_binary:           thrift_compact:           thrift_http:           grpc:     processors:     exporters:       logging:       otlp:         endpoint: tempo-simplest-gateway.observability.svc.cluster.local:8090         tls:           insecure: false           ca_file: "/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt"         auth:           authenticator: bearertokenauth         headers:           X-Scope-OrgID: "dev"     service:       extensions: [bearertokenauth]       pipelines:         traces:           receivers: [otlp, jaeger]           exporters: [otlp, logging] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata:   name: tempostack-traces-write rules:   - apiGroups:       - 'tempo.grafana.com'     resources:       - dev     resourceNames:       - traces     verbs:       - 'create' --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:   name: tempostack-traces roleRef:   apiGroup: rbac.authorization.k8s.io   kind: ClusterRole   name: tempostack-traces-write subjects:   - kind: ServiceAccount     name: dev-collector     namespace: observability EOF&lt;/code&gt;&lt;/pre&gt; &lt;div&gt; &lt;/div&gt; &lt;p&gt;We have successfully configured and deployed tracing data collection and storage. Now we can deploy an example application that will generate traces and report them to the OpenTelemetry collector:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;kubectl apply -f - &lt;&lt;EOF apiVersion: batch/v1 kind: Job metadata:   name: telemetrygen   namespace: observability   labels:     app: telmeetrygen spec:   ttlSecondsAfterFinished: 30   template:     spec:       restartPolicy: OnFailure       containers:       - name: telemetrygen         image: ghcr.io/open-telemetry/opentelemetry-collector-contrib/telemetrygen:v0.74.0         args: [traces, --otlp-endpoint=dev-collector:4317, --otlp-insecure, --duration=240s, --rate=4] EOF&lt;/code&gt;&lt;/pre&gt; &lt;div&gt; &lt;/div&gt; &lt;p&gt;The Jaeger UI can be accessed on the URL: https://tempo-simplest-gateway-observability.apps-crc.testing/api/traces/v1/dev/search. This URL is from my &lt;a href="https://developers.redhat.com/products/openshift-local/overview"&gt;local CRC cluster&lt;/a&gt;. The dev in the URL denotes the dev tenant (Figure 3).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_of_jaeger_ui_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_of_jaeger_ui_0.jpg?itok=S8-N8avh" width="600" height="304" alt="A screenshot of the Jaeger UI trace search." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;span class="field field--name-field-license field--type-entity-reference field--label-inline field__items"&gt; &lt;span class="field__label"&gt;License&lt;/span&gt; &lt;span class="rhd-media-licence field__item"&gt; under &lt;a href="https://www.apache.org/licenses/LICENSE-2.0" title="Apache 2.0"&gt;Apache 2.0&lt;/a&gt;. &lt;/span&gt; &lt;/span&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: The Jaeger UI trace search.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Deploying the Tempo operator is simple&lt;/h2&gt; &lt;p&gt;Deploying Tempo with the operator is a straightforward process and gives users a curated set of configuration options that are relevant for running Tempo on Kubernetes. The integration with OpenShift OAuth makes authentication simple and well integrated into the platform. The Tempo operator is a new project. We are looking forward to future releases to further improve user experience and give users exciting capabilities, such as TraceQL, span RED metrics, auto-tuning of configuration parameters, and automating Tempo operation.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/08/01/how-deploy-new-grafana-tempo-operator-openshift" title="How to deploy the new Grafana Tempo operator on OpenShift"&gt;How to deploy the new Grafana Tempo operator on OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Pavol Loffay</dc:creator><dc:date>2023-08-01T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.41.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/07/kogito-1-41-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/07/kogito-1-41-0-released.html</id><updated>2023-07-31T11:29:40Z</updated><content type="html">We are glad to announce that the Kogito 1.41.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Serverless Workflow Python support. See   * Upgrade Serverless Workflow SDK Java to 4.0.4 For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.30.0 artifacts are available at the . A detailed changelog for 1.41.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title>How to handle transactions in Node.js reference architecture</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/31/how-handle-transactions-nodejs-reference-architecture" /><author><name>Michael Dawson</name></author><id>9ebaf8e2-b7d9-40c7-b416-097082c4f22d</id><updated>2023-07-31T07:00:00Z</updated><published>2023-07-31T07:00:00Z</published><summary type="html">&lt;p&gt;In many applications, completing more than a single update as an atomic unit (transaction) is needed to preserve data integrity. This installment of the ongoing &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js Reference Architecture&lt;/a&gt; series covers the Node.js reference architecture team’s experience with integrating transactions into your application to satisfy this requirement.&lt;/p&gt; &lt;p&gt;Follow the series:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Part 1: &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;Overview of the Node.js reference architecture&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Part 2: &lt;a href="https://developer.ibm.com/blogs/nodejs-reference-architectire-pino-for-logging/"&gt;Logging in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency"&gt;Code consistency in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Part 4: &lt;a href="https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs"&gt;GraphQL in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Part 5: &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers"&gt;Building good containers&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Part 6: &lt;a href="https://developers.redhat.com/articles/2021/12/03/introduction-nodejs-reference-architecture-part-6-choosing-web-frameworks"&gt;Choosing web frameworks&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Part 7: &lt;a href="https://developers.redhat.com/articles/2022/03/02/introduction-nodejs-reference-architecture-part-7-code-coverage"&gt;Code Coverage&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Part 8: &lt;a href="https://developers.redhat.com/articles/2022/04/11/introduction-nodejs-reference-architecture-part-8-typescript"&gt;Typescript&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Part 9: &lt;a href="https://developers.redhat.com/articles/2022/08/09/8-elements-securing-nodejs-applications"&gt;Securing Node.js applications&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Part 10: &lt;a href="https://developers.redhat.com/articles/2022/11/03/nodejs-reference-architecture-part-10-accessibility"&gt;Accessibility&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Part 11: &lt;a href="https://developers.redhat.com/articles/2022/12/21/typical-development-workflows"&gt;Typical development workflows&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Part 12: &lt;a href="https://developers.redhat.com/articles/2023/02/22/installing-nodejs-modules-using-npm-registry"&gt;Npm development&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Part 13: &lt;a href="https://developers.redhat.com/articles/2023/03/21/how-investigate-7-common-problems-production#"&gt;Problem determination&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Part 14: &lt;a href="https://developers.redhat.com/articles/2023/07/27/introduction-nodejs-reference-architecture-testing"&gt;Testing&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Part 15: Transaction handling&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Ecosystem support for transactions&lt;/h2&gt; &lt;p&gt;Unlike the Java ecosystem, there are no well-established application servers with built in support for transactions and no JavaScript specific standards for transactions. Instead, Node.js applications either depend on the transaction support within &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/functional-components/databases.md"&gt;databases&lt;/a&gt; and their related Node.js clients and/or use patterns which are needed when business logic is spread over a number of microservices. These patterns include:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://www.educative.io/answers/what-is-the-two-phase-commit-protocol"&gt;2 phase commit&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://medium.com/trendyol-tech/saga-pattern-briefly-5b6cf22dfabc"&gt;Saga pattern&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Leveraging database support for transactions&lt;/h2&gt; &lt;p&gt;In the team's experience, if the updates that need to be completed atomically can be handled by a single Node.js component, and you are using a database with support for transactions that is the easiest case.&lt;/p&gt; &lt;p&gt;In some Node.js database clients, there is no specific support for transactions in the client, but that does not mean that transactions are not supported. Instead transactions are managed by using protocol level statements (for example BEGIN/COMMIT in SQL). &lt;a href="https://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt; is one of those databases. A simple example of using a transaction from the node-postgres client &lt;a href="https://node-postgres.com/features/transactions"&gt;documentation&lt;/a&gt; is as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;const { Pool } = require('pg') const pool = new Pool() const client = await pool.connect() try { await client.query('BEGIN') const queryText = 'INSERT INTO users(name) VALUES($1) RETURNING id' const res = await client.query(queryText, ['brianc']) const insertPhotoText = 'INSERT INTO photos(user_id, photo_url) VALUES ($1, $2)' const insertPhotoValues = [res.rows[0].id, 's3.bucket.foo'] await client.query(insertPhotoText, insertPhotoValues) await client.query('COMMIT') } catch (e) { await client.query('ROLLBACK') throw e } finally { client.release() } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the team's experience transactions work best with async/await versus generators, and it is good to have a try/catch around the rollback sections in addition to those which do the commit.&lt;/p&gt; &lt;h2&gt;The challenges of microservices&lt;/h2&gt; &lt;p&gt;In the team’s experience, the more common case is that a Node.js application incorporates a number of microservices, each of which might have a connection to the database or even may store data across different types or instances of datastores.&lt;/p&gt; &lt;p&gt;The challenges that come with implementing transactions with microservices are not unique to Node.js and the common approaches include:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://www.educative.io/answers/what-is-the-two-phase-commit-protocol"&gt;2 phase commit&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://medium.com/trendyol-tech/saga-pattern-briefly-5b6cf22dfabc"&gt;Saga pattern&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;These are also used in Node.js applications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2018/10/01/patterns-for-distributed-transactions-within-a-microservices-architecture#what_is_the_problem_"&gt;Patterns for distributed transactions within a microservices architecture&lt;/a&gt; provides a nice overview of the challenges introduced when managing transactions across microservices and these two patterns.&lt;/p&gt; &lt;p&gt;In the team's experience, Node.js applications will most often use the Saga pattern as opposed to two-phase commit, in part since the microservices may not share the same data store.&lt;/p&gt; &lt;p&gt;When two-phase commit is used, it will have to depend on external support from an underlying data store. Some databases offer support to help with implementing the two-phase commit technique, so read through the documentation for the database you are using if you are planning to use that technique.&lt;/p&gt; &lt;h2&gt;Learn more about Node.js reference architecture&lt;/h2&gt; &lt;p&gt;I hope that this quick overview of the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/functional-components/transaction-handling.md"&gt;transaction handling&lt;/a&gt; part of the Node.js reference architecture, along with the team discussions that led to that content, has been helpful, and that the information shared in the architecture helps you in your future implementations. We plan to cover new topics regularly for the Node.js reference architecture series. Until the next installment, we invite you to visit the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture repository&lt;/a&gt; on GitHub, where you will see the work we have done and future topics.&lt;/p&gt; &lt;p&gt;To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/31/how-handle-transactions-nodejs-reference-architecture" title="How to handle transactions in Node.js reference architecture"&gt;How to handle transactions in Node.js reference architecture&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Michael Dawson</dc:creator><dc:date>2023-07-31T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.16.9.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-16-9-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-16-9-final-released/</id><updated>2023-07-31T00:00:00Z</updated><published>2023-07-31T00:00:00Z</published><summary type="html">As mentioned in previous blog posts, we encourage all our users to upgrade to Quarkus 3 (it is an easy task with quarkus update). However we understand the migration can require some time so we will continue to maintain 2.16.x for a while. Today, we released Quarkus 2.16.9.Final, the ninth...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-07-31T00:00:00Z</dc:date></entry><entry><title type="html">How to Upload and Download Files with a Servlet</title><link rel="alternate" href="https://www.mastertheboss.com/java-ee/servlet-30/uploading-a-file-with-a-servlet/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/java-ee/servlet-30/uploading-a-file-with-a-servlet/</id><updated>2023-07-30T08:50:59Z</updated><content type="html">This article will illustrate how to upload files using the Jakarta Servlet API. We will also learn how to use a Servlet to download a File that is available remotely on the Server. Uploading Files with a Servlet By using Jakarta Servlet API it is pretty simple to upload a File without the need of ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>DevNation Day: Modern App Dev videos are now available</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/28/devnation-day-modern-app-dev-videos-are-now-available" /><author><name>Valentina Rodriguez Sosa</name></author><id>e588c9d2-ed9d-44b6-ad76-67949bc05ad7</id><updated>2023-07-28T13:00:00Z</updated><published>2023-07-28T13:00:00Z</published><summary type="html">&lt;p&gt;Last month Red Hat Developer hosted &lt;a href="https://developers.redhat.com/devnation/dnd"&gt;DevNation Day: Modern App Dev&lt;/a&gt;. This one-day virtual event brought together application developers, DevOps and platform engineers, enterprise architects, and application managers from across the globe.&lt;/p&gt; &lt;p&gt;Attendees participated in hands-on labs and got expert advice from Red Hat practitioners on best practices, technologies, and architectures advancing the state of modern application development on the hybrid cloud with &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. The event featured talks on app modernization, &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;APIs&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/serverless-architecture/"&gt;serverless&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/security/"&gt;security&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt;, &lt;a href="https://developers.redhat.com/articles/2023/05/23/developers-guide-red-hat-developer-hub-and-janus/"&gt;internal developer platforms&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/red-hat-architecture-and-design-patterns"&gt;application architecture&lt;/a&gt;, and much more.&lt;/p&gt; &lt;p&gt;The videos from DevNation Day: Modern App Dev are now available for you to watch online anytime. Read on to explore some of the highlights, or &lt;a href="https://www.youtube.com/playlist?list=PLf3vm0UK6HKpNrHaILLCMitayjfnCAy41"&gt;jump right over to the playlist&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Keynote: Becoming the developer's developer&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Speaker:&lt;/strong&gt; &lt;a href="https://developers.redhat.com/authors/burr-sutter/"&gt;Burr Sutter, Director of Developer Experience&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In his DevNation Day keynote &lt;a href="https://www.youtube.com/watch?v=TRE2lJwsDlk"&gt;Becoming the developer's developer&lt;/a&gt;, Burr presents challenges and opportunities for teams delivering software to production today. This engaging talk covers a range of topics including platform engineering, &lt;a href="https://developers.redhat.com/topics/devsecops"&gt;DevSecOps&lt;/a&gt;, software as a team effort, and the importance of reducing cognitive load for developers and providing the tools they need to build better applications. He also shares resources to help you build an understanding of what it means to build better software within your organization. &lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;Breaking silo with Ansible&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Session:&lt;/strong&gt; &lt;a href="https://www.youtube.com/watch?v=6-bcVni0ej8&amp;list=PLf3vm0UK6HKpNrHaILLCMitayjfnCAy41&amp;index=1"&gt;Breaking silo with Ansible&lt;/a&gt;&lt;br /&gt;&lt;strong&gt;Speaker:&lt;/strong&gt; &lt;a href="https://developers.redhat.com/author/romain-pelisse"&gt;Roman Pelisse, Senior Software Engineer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The effective operation of software infrastructure requires rigorous &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt;, regardless of where it is hosted: on-premise, in the cloud, at the edge, and everything in between. Each step an application takes throughout its software development life cycle on its way from development to production should be managed in an automated fashion. Organizations have traditionally struggled to adopt a holistic automation approach due to the division between development and operations.&lt;/p&gt; &lt;p&gt;In this session, Romain Pelisse demonstrates how &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Ansible&lt;/a&gt; can become the lingua franca to help break the walls between these organizational units and drive real business value.&lt;/p&gt; &lt;h2&gt;Quarkus for Spring developers&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Session:&lt;/strong&gt; &lt;a href="https://www.youtube.com/watch?v=AxNiHJAbJSc&amp;list=PLf3vm0UK6HKpNrHaILLCMitayjfnCAy41&amp;index=13"&gt;Quarkus for Spring developers&lt;/a&gt;&lt;br /&gt;&lt;strong&gt;Speaker:&lt;/strong&gt; &lt;a href="https://developers.redhat.com/author/eric-deandrea"&gt;Eric Deandrea, Senior Principal Technical Marketing Manager&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In this session, Eric Deandrea discusses concepts and conventions familiar to &lt;a href="https://developers.redhat.com/topics/spring-boot/"&gt;Spring&lt;/a&gt; developers and how they can be implemented in Quarkus, explaining the similarities and differences between them. You'll also explore similarities and differences in how &lt;a href="https://developers.redhat.com/articles/2021/11/08/test-driven-development-quarkus"&gt;testing&lt;/a&gt; is done, with a look at Quarkus Dev Services and continuous testing.&lt;/p&gt; &lt;p&gt;This session is focused on live coding; Eric takes an existing Spring application with a full test suite and builds a Quarkus equivalent version of it live. &lt;/p&gt; &lt;p class="Indent1"&gt;[ &lt;strong&gt;Get the e-book: &lt;a href="https://developers.redhat.com/e-books/quarkus-spring-developers"&gt;Quarkus for Spring Developers&lt;/a&gt;&lt;/strong&gt; ]&lt;/p&gt; &lt;h2&gt;Simplifying the inner and outer loop&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Session:&lt;/strong&gt; &lt;a href="https://www.youtube.com/watch?v=6-3xSNqwlG4&amp;list=PLf3vm0UK6HKpNrHaILLCMitayjfnCAy41&amp;index=6"&gt;Java: Life is short&lt;/a&gt;&lt;br /&gt;&lt;strong&gt;Speaker:&lt;/strong&gt; &lt;a href="https://developers.redhat.com/author/kevin-dubois"&gt;Kevin Dubois, Principal Developer Advocate&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How can you speed up your development and spend time on things that are actually interesting instead of waiting for code compilations and deployments, fidgeting with configurations, or spending hours tracking down bugs on production?&lt;/p&gt; &lt;p&gt;In this talk, Developer Advocate Kevin Dubois takes you through the inner and outer development loop and shows you some interesting ways to simplify and accelerate development along the way. He provides examples and explanations to demonstrate an opinionated set of open source projects that can make life as a developer more enjoyable and productive.&lt;/p&gt; &lt;h2&gt;A new way to work with containers and Kubernetes&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Session: &lt;/strong&gt;&lt;a href="https://www.youtube.com/watch?v=p1InzhcyCYE&amp;list=PLf3vm0UK6HKpNrHaILLCMitayjfnCAy41&amp;index=5"&gt;Containers to pods to Kubernetes: Podman Desktop&lt;/a&gt;&lt;br /&gt;&lt;strong&gt;Speaker:&lt;/strong&gt; &lt;a href="https://developers.redhat.com/author/stevan-le-meur"&gt;Stévan Le Meur, Principal Product Manager&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kubernetes has skyrocketed to the top as the premier platform for managing containers at scale. But let's face it—working with Kubernetes as a developer can be intimidating, especially if you're just starting their containerization journey. Local development environments tend to lack consistency with target Kubernetes environments, causing challenges in deploying and debugging your applications in production.&lt;/p&gt; &lt;p&gt;So how can you minimize this inconsistency as a developer and smoothly transition your application from your development desktop into production? Podman Desktop is the answer!&lt;/p&gt; &lt;p&gt;This talk from Stévan Le Meur introduces Podman Desktop, a powerful, cross-platform and open source graphical tool that simplifies container development workflows. He&lt;span&gt; explains how this cutting-edge tool can guide you through the journey from application to containers, to pods, and finally to Kubernetes. This is a must-see for anyone looking to streamline their container development process.&lt;/span&gt;&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;[ Learn more: &lt;a href="https://developers.redhat.com/articles/2023/03/01/podman-desktop-introduction"&gt;What is Podman Desktop? A developer's introduction&lt;/a&gt; ]&lt;/strong&gt;&lt;/p&gt; &lt;h2&gt;Fine-grained API authorization &lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Session: &lt;/strong&gt;&lt;a href="https://www.youtube.com/watch?v=L4wHQwY-tIA&amp;list=PLf3vm0UK6HKpNrHaILLCMitayjfnCAy41&amp;index=9"&gt;Fine-grained API Authorization using 3scale and authorization systems&lt;/a&gt;&lt;br /&gt;&lt;strong&gt;Speaker:&lt;/strong&gt; Abdelhamid Soliman, Senior Specialist Solution Architect, Application Services&lt;/p&gt; &lt;p&gt;In this talk, Abdelhamid Soliman explores the importance of fine-grained API authorization and how it can be achieved using Red Hat 3scale API Management and authorization systems such as Keycloak and Open Policy Agent (OPA). This talk covers the various types of authorization mechanisms and their benefits, as well as the challenges of implementing fine-grained authorization.&lt;/p&gt; &lt;h2&gt;Serverless computing: Benefits and drawbacks&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Session: &lt;/strong&gt;&lt;a href="https://www.youtube.com/watch?v=GSLxrsU4fdQ&amp;list=PLf3vm0UK6HKpNrHaILLCMitayjfnCAy41&amp;index=19"&gt;Serverless computing: Explore the benefits and drawbacks of serverless computing and how to build serverless applications&lt;/a&gt;&lt;br /&gt;&lt;strong&gt;Speakers&lt;/strong&gt;: Steve Tran, Principal Consultant, and Jon Keam, Associate Principal Specialist Solution Architect, Application Platform&lt;/p&gt; &lt;p&gt;In this session, Steve Tran and Jon Keam discuss the advantages of serverless, including reduced costs, increased scalability, and faster development times. They explain the limitations of distributed monoliths and how the shift toward cloud and microservice architecture has highlighted these drawbacks. &lt;/p&gt; &lt;p&gt;Explore the benefits of using Quarkus to modernize legacy workloads, including streamlining development, enhancing performance, and lowering operational costs. This talk also provides valuable insights and considerations for organizations planning to modernize legacy workloads with Quarkus, ensuring a smooth and successful transition toward a more agile and scalable application infrastructure.&lt;/p&gt; &lt;h2&gt;Even more DevNation Day&lt;/h2&gt; &lt;p&gt;You can find more great sessions in our &lt;a href="https://www.youtube.com/playlist?list=PLf3vm0UK6HKpNrHaILLCMitayjfnCAy41"&gt;video playlist&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Visit the &lt;a href="https://developers.redhat.com/devnation/dnd"&gt;DevNation Day&lt;/a&gt; page on Red Hat Developer for upcoming events and even more video content. To see all upcoming DevNation events, check out the &lt;a href="https://developers.redhat.com/devnation#assembly-field-sections-85041"&gt;DevNation events calendar&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/28/devnation-day-modern-app-dev-videos-are-now-available" title="DevNation Day: Modern App Dev videos are now available"&gt;DevNation Day: Modern App Dev videos are now available&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Valentina Rodriguez Sosa</dc:creator><dc:date>2023-07-28T13:00:00Z</dc:date></entry></feed>
